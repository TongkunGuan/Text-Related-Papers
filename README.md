# Text-Related-Papers
This repository contains a paper collection of the latest text-related papers from top conferences.

## üìñ Table of Contents
- [Text Recognition](#Text-Recognition)
- [Controllable Text Generation](#text-to-image)
- [Multi-modal Large Language Model](#Multi-modal-Large-Language-Model)
- [Text Detection](#text-detection)
- [GUI Agents](#UI-understanding)
##

ÊñáÊ°£Â§öÊ®°ÊÄÅÂ§ßÊ®°Âûã Survey: https://arxiv.org/pdf/2502.16586

<details open>
<summary>üëÄ </summary>

### GUI Agents
+ [OmniParser for Pure Vision Based GUI Agent](https://arxiv.org/pdf/2408.00203) (08 Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/microsoft/OmniParser)

+ [UI-Hawk: Unleashing the Screen Stream Understanding for GUI Agents](https://www.preprints.org/manuscript/202408.2137/v1) (08 Arxiv)

### Text Recognition

+ [PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer](https://arxiv.org/pdf/2407.07764) (ECCV2024)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/SJTU-DeepVisionLab/PosFormer)
  
+ [Self-supervised Character-to-Character Distillation for Text Recognition](https://arxiv.org/pdf/2211.00288.pdf) (ICCV 2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/TongkunGuan/CCD)
  
+ [MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition](https://arxiv.org/abs/2305.14758) (ICCV 2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/simplify23/MRN)
  
+ [Revisiting Scene Text Recognition: A Data Perspective](https://arxiv.org/abs/2307.08723) (ICCV 2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/Mountchicken/Union14M)
  
+ [Self-Supervised Implicit Glyph Attention for Text Recognition](https://openaccess.thecvf.com/content/CVPR2023/html/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.html) (CVPR 2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/TongkunGuan/SIGA)
  
+ [Relational Contrastive Learning for Scene Text Recognition](https://arxiv.org/pdf/2308.00508.pdf) (ACMMM 2023) [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ThunderVVV/RCLSTR)
  
+ [TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition](https://arxiv.org/abs/2305.05322) (IJCAI 2023)[![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/simplify23/TPS_PP)
  
+ [Linguistic More: Taking a Further Step toward Efficient and Accurate Scene Text Recognition](https://arxiv.org/pdf/2305.05140.pdf) (IJCAI 2023)[![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/CyrilSterling/LPV)
  
+ [Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition](https://dl.acm.org/doi/abs/10.1145/3503161.3547784) (ACMMM 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ayumiymk/DiG)
  
+ [Chinese Character Recognition with Augmented Character Profile Matching](https://dl.acm.org/doi/abs/10.1145/3503161.3547827) (ACMMM 2022)  
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/FudanVI/FudanOCR/tree/main/character-profile-matching)
  
+ [Scene Text Recognition with Permuted Autoregressive Sequence Models](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_11) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/baudm/parseq)
  
+ [Task Grouping for Multilingual Text Recognition (Workshops)](https://link.springer.com/chapter/10.1007/978-3-031-25069-9_20) (ECCV 2022 Workshops)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Multi-modal Text Recognition Networks: Interactive Enhancements Between Visual and Semantic Features](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_26) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/wp03052/MATRN)
  
+ [On Calibration of Scene-Text Recognition Models (Workshops)](https://link.springer.com/chapter/10.1007/978-3-031-25069-9_18) (ECCV 2022 Workshops)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Pure Transformer with Integrated Experts for Scene Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_28) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_14) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Multi-granularity Prediction for Scene Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_20) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR)
  
+ [Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_18) (ECCV 2022) [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/xdxie/WordArt)
  
+ [Background-Insensitive Scene Text Recognition with Text Semantic Segmentation](https://link.springer.com/chapter/10.1007/978-3-031-19806-9_10) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [SGBANet: Semantic GAN and Balanced Attention Network for Arbitrarily Oriented Scene Text Recognition](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_27) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Levenshtein OCR](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_19) (ECCV 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/LevOCR)
  
+ [SVTR: Scene Text Recognition with a Single Visual Model](https://arxiv.org/abs/2205.00159) (IJCAI 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/PaddlePaddle/PaddleOCR)
  
+ [Open-Set Text Recognition via Character-Context Decoupling](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Open-Set_Text_Recognition_via_Character-Context_Decoupling_CVPR_2022_paper.pdf) (CVPR 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/lancercat/VSDF)
  
+ [Knowledge Mining with Scene Text for Fine-Grained Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Knowledge_Mining_With_Scene_Text_for_Fine-Grained_Recognition_CVPR_2022_paper.pdf) (CVPR 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/MCLAB-OCR/KnowledgeMiningWithSceneText)
  
+ [Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/19971) (AAAI 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/adeline-cs/GTR)
  
+ [Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20062) (AAAI 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Context-Based Contrastive Learning for Scene Text Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20245) (AAAI 2022)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [Sequence-to-Sequence Contrastive Learning for Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Aberdam_Sequence-to-Sequence_Contrastive_Learning_for_Text_Recognition_CVPR_2021_paper.pdf) (CVPR 2021)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com)
  
+ [What if We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Baek_What_if_We_Only_Use_Real_Datasets_for_Scene_Text_CVPR_2021_paper.pdf) (CVPR 2021)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ku21fan/STR-Fewer-Labels)
  
+ [MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhunia_MetaHTR_Towards_Writer-Adaptive_Handwritten_Text_Recognition_CVPR_2021_paper.pdf) (CVPR 2021)  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/tobiasvanderwerff/MetaHTR)
  
+ [Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.pdf) (CVPR 2021) [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/FangShancheng/ABINet)
  
+ [Dictionary-Guided Scene Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.pdf) (CVPR 2021) [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/VinAIResearch/dict-guided)
  
+ [Primitive Representation Learning for Scene Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Primitive_Representation_Learning_for_Scene_Text_Recognition_CVPR_2021_paper.pdf) (CVPR 2021)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/RuijieJ/pren)
</details>

<details open>
<summary>üëÄ </summary>

### Controllable Text Generation

+ [How To Create SOTA Image Generation with Text: Recraft‚Äôs ML Team Insights](https://www.recraft.ai/blog/how-to-create-sota-image-generation-with-text-recrafts-ml-team-insights)

+ [Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering](https://arxiv.org/pdf/2403.09622) (ECCV2024) 
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://glyph-byt5.github.io/)

+ [Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering](https://arxiv.org/pdf/2406.10208) (ECCV2024)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://glyph-byt5-v2.github.io/)

+ [ANYTEXT: MULTILINGUAL VISUAL TEXT GENERATION AND EDITING](https://arxiv.org/pdf/2311.03054) (ICLR2024)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/tyxsspa/AnyText)

+ [Character-Aware Models Improve Visual Text Rendering](https://arxiv.org/pdf/2212.10562) (ACL2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)]()

+ [TextDiffuser: Diffusion Models as Text Painters](https://arxiv.org/pdf/2305.10855) (NeurIPS2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://aka.ms/textdiffuser)

+ [GlyphControl: Glyph Conditional Control for Visual Text Generation](https://arxiv.org/pdf/2305.18259) (NeurIPS2023)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/AIGText/GlyphControl-release)

+ [Layout-Agnostic Scene Text Image Synthesis with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhangli_Layout-Agnostic_Scene_Text_Image_Synthesis_with_Diffusion_Models_CVPR_2024_paper.pdf) (CVPR2024)

+ [CustomText: Customized Textual Image Generation using Diffusion Models](https://arxiv.org/pdf/2405.12531) (CVPR2024)

+ [TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering](https://arxiv.org/pdf/2311.16465) (Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://aka.ms/textdiffuser-2)

+ [Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation](https://arxiv.org/pdf/2403.16422) (Arxiv)

+ [Typographic Text Generation with Off-the-Shelf Diffusion Model](https://arxiv.org/pdf/2402.14314) (Arxiv)

+ [High Fidelity Scene Text Synthesis](https://arxiv.org/pdf/2405.14701) (Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/CodeGoat24/DreamText)

+ [UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models](https://arxiv.org/abs/2312.04884) (Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ZYM-PKU/UDiffText)

+ [GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion Models and Large Language Models](https://arxiv.org/pdf/2407.02252) (Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/OPPO-Mente-Lab/GlyphDraw2)

+ [ARTIST: Improving the Generation of Text-rich Images by Disentanglement](https://arxiv.org/pdf/2406.12044) (Arxiv)
</details>

<details open>
<summary>üëÄ </summary>
  
### Multi-modal Large Language Model

+ [On Pre-training of Multimodal Language Models Customized for Chart Understanding]() (06 Arxiv)

+ [PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling]() (10 Arxiv)

+ [MPLUG-DOCOWL2: HIGH-RESOLUTION COMPRESSING FOR OCR-FREE MULTI-PAGE DOCUMENT UNDERSTANDING]() (09 Arxiv)

+ [LayTextLLM: A Bounding Box is Worth One Token - Interleaving Layout and Text in a Large Language Model for Document Understanding]() (06 Arxiv)
  
+ [RegionGPT: Towards Region Understanding Vision Language Model]() (05 Arxiv)
  
+ [Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding](https://arxiv.org/pdf/2411.05254) (NeurIPS 2024)

+ [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://openaccess.thecvf.com/content/CVPR2024/papers/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.pdf) (CVPR24)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/kakaobrain/honeybee)

+ [Continuous or Discrete, That Is the Question: A Survey on Large MultiModal Models from the Perspective of Input-Output Space Extension](10.20944/preprints202411.0685.v1) (11 Arxiv)

+ [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/pdf/2402.03766) (01 Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/Meituan-AutoML/MobileVLM)

+ [InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model
Handling Resolutions from 336 Pixels to 4K HD](https://arxiv.org/pdf/2404.06512) (04 Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/InternLM/InternLM-XComposer)
  
+ [Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive Cropping](https://arxiv.org/pdf/2408.02034) (08 Arxiv)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/Yuliang-Liu/Monkey)
  
+ [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_CVPR_2024_paper.pdf) (CVPR2024)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/OpenGVLab/InternVL)

+ [TRINS: Towards Multimodal Language Models that Can Read](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRINS_Towards_Multimodal_Language_Models_that_Can_Read_CVPR_2024_paper.pdf) (CVPR2024)
  [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/puar-playground/MMR_Bench)

+ [UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://arxiv.org/pdf/2310.05126) (EMNLP2023)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/LukeForeverYoung/UReader)

+ [On Pre-training of Multimodal Language Models Customized for Chart Understanding](https://arxiv.org/pdf/2407.14506) (Arxiv)

+ [LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models](https://arxiv.org/pdf/2407.19185) (Arxiv)

+ [Multimodal Table Understanding](https://arxiv.org/pdf/2406.08100) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/SpursGoZmy/Table-LLaVA)

+ [Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding](https://arxiv.org/pdf/2407.14439) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/JiuTian-VL/TokenCorrCompressor)

+ [LayTextLLM: A Bounding Box is Worth One Token - Interleaving Layout and Text in a Large Language Model for Document Understanding](https://arxiv.org/pdf/2407.01976) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/LayTextLLM/LayTextLLM)

+ [MoAI: Mixture of All Intelligence for Large Language and Vision Models](https://arxiv.org/abs/2403.07508) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ByungKwanLee/MoAI)

+ [Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning](https://arxiv.org/abs/2406.02547) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://fingerrec.github.io/visincontext/)

+ [TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document](https://arxiv.org/abs/2403.04473) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/Yuliang-Liu/Monkey)

+ [DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding](https://arxiv.org/pdf/2311.11810) (Arxiv)

+ [Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/pdf/2312.06109) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://varybase.github.io/)

+ [Fox: Focus Anywhere for Fine-grained Multi-page Document Understanding](https://arxiv.org/abs/2405.14295) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/Ucas-HaoranWei/Fox)

+ [TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](https://arxiv.org/abs/2404.09204) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/yuyq96/TextHawk)

+ [mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding](https://arxiv.org/abs/2403.12895) (Arxiv)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5)

+ [DOCLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING]() (12-2023 Arxiv)
</details>

<details open>
<summary>üëÄ </summary>
  
### Text Detection
+ [Bridging Synthetic and Real Worlds for Pre-training Scene Text Detector](https://arxiv.org/pdf/2312.05286) (ECCV2024)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/SJTU-DeepVisionLab/FreeReal)

+ [LORE: Logical Location Regression Network for Table Structure Recognition](https://arxiv.org/pdf/2303.03730.pdf) (AAAI2024)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LORE-TSR)

+ [LRANet: Towards Accurate and Efficient Scene Text Detection with Low-Rank Approximation Network](https://arxiv.org/abs/2306.15142) (AAAI2024)
 [![Code](https://img.shields.io/badge/GitHub-9cf?style=flat&logo=github&logoColor=black)](https://github.com/ychensu/LRANet)

+ [CPN: Complementary Proposal Network for Unconstrained Text Detection](https://arxiv.org/pdf/2402.11540.pdf) (AAAI2024)
</details>


