**Visual Foundation Model (VFM)**

# General Foundation Models

- **CLIP**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

- **DINO**: [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

- **SAM**: [Segment Anything](https://arxiv.org/abs/2304.02643)

- **SigLIP**: [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)

- **DINOv2**: [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

- **RADIO**: [AM-RADIO: Agglomerative Vision Foundation Model](https://arxiv.org/abs/2301.13254)

- **UINT**: [UNIT: Unifying Image and Text Recognition in One Vision Encoder](https://arxiv.org/abs/2203.05765)

- **SigLIP2**: [SigLIP 2: A better multilingual vision language encoder](https://arxiv.org/pdf/2502.14786?)

- **InternVIT**: [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/abs/2211.06387)

- **QwenViT**: [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)

- **UniCOM**: [Unicom: Universal and Compact Representation Learning for Image Retrieval](https://arxiv.org/abs/2303.09328)

# Expert Foundation Models

- **LayoutLM**: [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)

- **Platypus**: [A Generalized Specialist Model for Reading Text in Various Forms](https://arxiv.org/abs/2302.14895)

- **InstructOCR**: [InstructOCR: Instruction Boosting Scene Text Spotting](https://arxiv.org/abs/2304.08626)

- **ODM**: [A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting](https://arxiv.org/abs/2305.12698)

- **TokenFD**: [A Token-level Text Image Foundation Model for Document Understanding](https://arxiv.org/abs/2503.02304)

  

