# **Visual Foundation Model (VFM)**

## General Foundation Models

- **CLIP**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

- **DINO**: [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

- **SAM**: [Segment Anything](https://arxiv.org/abs/2304.02643)

- **SigLIP**: [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)

- **DINOv2**: [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

- **RADIO**: [AM-RADIO: Agglomerative Vision Foundation Model](https://arxiv.org/abs/2301.13254)

- **UINT**: [UNIT: Unifying Image and Text Recognition in One Vision Encoder](https://arxiv.org/abs/2203.05765)

- **SigLIP2**: [SigLIP 2: A better multilingual vision language encoder](https://arxiv.org/pdf/2502.14786?)

- **InternVIT**: [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/abs/2211.06387)

- **QwenViT**: [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)

- **UniCOM**: [Unicom: Universal and Compact Representation Learning for Image Retrieval](https://arxiv.org/abs/2303.09328)

## Expert Foundation Models

- **LayoutLM**: [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)

- **Platypus**: [A Generalized Specialist Model for Reading Text in Various Forms](https://arxiv.org/abs/2302.14895)

- **InstructOCR**: [InstructOCR: Instruction Boosting Scene Text Spotting](https://arxiv.org/abs/2304.08626)

- **ODM**: [A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting](https://arxiv.org/abs/2305.12698)

- **TokenFD**: [A Token-level Text Image Foundation Model for Document Understanding](https://arxiv.org/abs/2503.02304)

# **Training Strategy**

## Data preparationï¼š

1) public dataset (VQA datasets and OCR datasets) 

2) self-constructed generated pipeline [1](https://arxiv.org/abs/2406.06730) [2](https://arxiv.org/abs/2403.00816)

## Modality alignment:

1)  Predict full texts [1](https://arxiv.org/abs/2307.02499) 2) Text grounding [1](https://arxiv.org/abs/2309.11419) 3) Predict part texts [1]() 4) Format conversion [1](https://arxiv.org/abs/2307.02499) ...

## Instruct alignment:

1) VQA tuning 2) Chain of Thought (CoT) [1](https://arxiv.org/abs/2403.00816) [2](https://arxiv.org/abs/2403.16999) 3) Retrieval-Augmented Generation (RAG) [1](https://arxiv.org/pdf/2410.10594) [2](https://arxiv.org/pdf/2407.01449)

## Preference Alignment

1) DPO 2) GRPO [1](https://arxiv.org/abs/2402.03300) 3) MPO [1](https://arxiv.org/abs/2411.10442)



  

